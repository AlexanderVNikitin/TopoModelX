{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypergraph Neural Network\n",
    "\n",
    "In this notebook, we will create and train a two-step message passing network in the hypergraph domain. We will use a benchmark dataset, shrec16, a collection of 3D meshes, to train the model to perform classification at the level of the hypergraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.222779223Z",
     "start_time": "2023-06-01T16:14:49.575421023Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_geometric/typing.py:33: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: dlopen(/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_scatter/_version_cpu.so, 0x0006): tried: '/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_scatter/_version_cpu.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e'))\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_geometric/typing.py:64: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_sparse/_version_cpu.so, 0x0006): tried: '/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_sparse/_version_cpu.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e'))\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from toponetx import SimplicialComplex\n",
    "import toponetx.datasets as datasets\n",
    "from topomodelx.nn.hypergraph.allset_layer import AllSetLayer\n",
    "\n",
    "from torch_geometric.utils import to_edge_index\n",
    "# make ipynb to read .py files continiously\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:51.959770754Z",
     "start_time": "2023-06-01T16:14:51.956096841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "The first step is to import the dataset, shrec 16, a benchmark dataset for 3D mesh classification. We then lift each graph into our domain of choice, a hypergraph.\n",
    "\n",
    "We will also retrieve:\n",
    "- input signal on the edges for each of these hypergraphs, as that will be what we feed the model in input\n",
    "- the label associated to the hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:53.022151550Z",
     "start_time": "2023-06-01T16:14:52.949636599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6th simplicial complex has 252 nodes with features of dimension 6.\n",
      "The 6th simplicial complex has 750 edges with features of dimension 10.\n",
      "The 6th simplicial complex has 500 faces with features of dimension 7.\n"
     ]
    }
   ],
   "source": [
    "i_complex = 6\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_0s[i_complex].shape[0]} nodes with features of dimension {x_0s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_1s[i_complex].shape[0]} edges with features of dimension {x_1s[i_complex].shape[1]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The {i_complex}th simplicial complex has {x_2s[i_complex].shape[0]} faces with features of dimension {x_2s[i_complex].shape[1]}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures and lift into hypergraph domain. ##\n",
    "\n",
    "Now we retrieve the neighborhood structures (i.e. their representative matrices) that we will use to send messges on each simplicial complex. In the case of this architecture, we need the boundary matrix (or incidence matrix) $B_1$ with shape $n_\\text{nodes} \\times n_\\text{edges}$.\n",
    "\n",
    "Once we have recorded the incidence matrix (note that all incidence amtrices in the hypergraph domain must be unsigned), we lift each simplicial complex into a hypergraph. The pairwise edges will become pairwise hyperedges, and faces in the simplciial complex will become 3-wise hyperedges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:53.022151550Z",
     "start_time": "2023-06-01T16:14:52.949636599Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/kdqr0d7n3y1cyygtb_y421tc0000gn/T/ipykernel_18771/2785629712.py:14: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:56.)\n",
      "  incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse_csr()\n"
     ]
    }
   ],
   "source": [
    "hg_list = []\n",
    "incidence_1_list = []\n",
    "for simplex in simplexes:\n",
    "    incidence_1 = simplex.incidence_matrix(rank=1, signed=False)\n",
    "    # incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "    # incidence_1_list.append(incidence_1)\n",
    "    hg = simplex.to_hypergraph()\n",
    "    hg_list.append(hg)\n",
    "\n",
    "\n",
    "# Extract hypergraphs incident matrices from collected hypergraphs\n",
    "for hg in hg_list:\n",
    "    incidence_1 = hg.incidence_matrix()\n",
    "    incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse_csr()\n",
    "    incidence_1_list.append(incidence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6th hypergraph has an incidence matrix of shape torch.Size([252, 1250]).\n"
     ]
    }
   ],
   "source": [
    "i_complex = 6\n",
    "print(\n",
    "    f\"The {i_complex}th hypergraph has an incidence matrix of shape {incidence_1_list[i_complex].shape}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the TemplateLayer class, we create a neural network with stacked layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:55.343005145Z",
     "start_time": "2023-06-01T16:14:55.339481459Z"
    }
   },
   "outputs": [],
   "source": [
    "channels_edge = x_1s[0].shape[1]\n",
    "channels_node = x_0s[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:56.033274119Z",
     "start_time": "2023-06-01T16:14:56.029056913Z"
    }
   },
   "outputs": [],
   "source": [
    "class AllSetNN(torch.nn.Module):\n",
    "    \"\"\"AllSet Neural Network Module.\n",
    "\n",
    "    A module that combines multiple AllSet layers to form a neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_dim : int\n",
    "        Dimension of the input features.\n",
    "    hid_dim : int\n",
    "        Dimension of the hidden features.\n",
    "    out_dim : int\n",
    "        Dimension of the output features.\n",
    "    dropout : float\n",
    "        Dropout probability.\n",
    "    n_layers : int, optional\n",
    "        Number of AllSet layers in the network. Defaults to 2.\n",
    "    input_dropout : float, optional\n",
    "        Dropout probability for the layer input. Defaults to 0.2.\n",
    "    mlp_num_layers : int, optional\n",
    "        Number of layers in the MLP. Defaults to 2.\n",
    "    mlp_input_norm : bool, optional\n",
    "        Whether to apply input normalization in the MLP. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, hid_dim, out_dim,\n",
    "                 n_layers=2, dropout=0.2, input_dropout=0.2,\n",
    "                 mlp_num_layers=2, mlp_input_norm=False):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            AllSetLayer(\n",
    "                in_dim=in_dim,\n",
    "                hid_dim=hid_dim,\n",
    "                out_dim=hid_dim,\n",
    "                dropout=dropout,\n",
    "                input_dropout=input_dropout,\n",
    "                mlp_num_layers=mlp_num_layers,\n",
    "                mlp_input_norm=mlp_input_norm\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(\n",
    "                AllSetLayer(\n",
    "                    in_dim=hid_dim,\n",
    "                    hid_dim=hid_dim,\n",
    "                    out_dim=hid_dim,\n",
    "                    dropout=dropout,\n",
    "                    input_dropout=input_dropout,\n",
    "                    mlp_num_layers=mlp_num_layers,\n",
    "                    mlp_input_norm=mlp_input_norm\n",
    "                )\n",
    "            )\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.linear = torch.nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x_0, incidence_1):\n",
    "        \"\"\"\n",
    "        Forward computation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input features.\n",
    "        edge_index : torch.Tensor\n",
    "            Edge list (of size (2, |E|)).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output prediction.\n",
    "        \"\"\"\n",
    "        # cidx = edge_index[1].min()\n",
    "        # edge_index[1] -= cidx\n",
    "        # reversed_edge_index = torch.stack(\n",
    "        #     [edge_index[1], edge_index[0]], dim=0)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x_0 = layer(x_0, incidence_1)\n",
    "        pooled_x = torch.max(x_0, dim=0)[0]\n",
    "        return torch.sigmoid(self.linear(pooled_x))[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model, the loss, and an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:58.153514385Z",
     "start_time": "2023-06-01T16:14:57.243596119Z"
    }
   },
   "outputs": [],
   "source": [
    "#model = AllSetNN(channels_edge, channels_node, n_layers=2)\n",
    "# ADD edge channels as well, it seems to be necessary for hypergraph NNs\n",
    "hid_dim, out_dim = 64, 1\n",
    "model = AllSetNN(in_dim=channels_node, hid_dim=hid_dim, out_dim=out_dim, n_layers=1)\n",
    "model = model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:14:59.046068930Z",
     "start_time": "2023-06-01T16:14:59.037648626Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_0_train, x_0_test = train_test_split(x_0s, test_size=test_size, shuffle=False)\n",
    "incidence_1_train, incidence_1_test = train_test_split(\n",
    "    incidence_1_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T16:15:01.683216142Z",
     "start_time": "2023-06-01T16:15:00.727075750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 274.9095\n",
      "Epoch: 2 loss: 274.6125\n",
      "Epoch: 3 loss: 274.6125\n",
      "Epoch: 4 loss: 274.6125\n",
      "Epoch: 5 loss: 274.6125\n",
      "Epoch: 6 loss: 274.6125\n",
      "Epoch: 7 loss: 274.6125\n",
      "Epoch: 8 loss: 274.6125\n",
      "Epoch: 9 loss: 274.6125\n",
      "Epoch: 10 loss: 274.6125\n",
      "Test_loss: 529.0000\n",
      "Epoch: 11 loss: 274.6125\n",
      "Epoch: 12 loss: 274.6125\n",
      "Epoch: 13 loss: 274.6125\n",
      "Epoch: 14 loss: 274.6125\n",
      "Epoch: 15 loss: 274.6125\n",
      "Epoch: 16 loss: 274.6125\n",
      "Epoch: 17 loss: 274.6125\n",
      "Epoch: 18 loss: 274.6125\n",
      "Epoch: 19 loss: 274.6125\n",
      "Epoch: 20 loss: 274.6125\n",
      "Test_loss: 529.0000\n",
      "Epoch: 21 loss: 274.6125\n",
      "Epoch: 22 loss: 274.6125\n",
      "Epoch: 23 loss: 274.6125\n",
      "Epoch: 24 loss: 274.6125\n",
      "Epoch: 25 loss: 274.6125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m y_hat \u001b[39m=\u001b[39m model(x_0, incidence_1)\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_hat, y)\n\u001b[0;32m---> 19\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     20\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m epoch_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/topo/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/topo/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_interval = 10\n",
    "num_epochs = 50\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for x_0, incidence_1, y in zip(x_0_train, incidence_1_train, y_train):\n",
    "        x_0= torch.tensor(x_0)\n",
    "        x_0, incidence_1, y = (\n",
    "            x_0.float().to(device),\n",
    "            incidence_1.float().to(device),\n",
    "            torch.tensor(y, dtype=torch.float).to(device),\n",
    "        )\n",
    "        opt.zero_grad()\n",
    "        # Extract edge_index from sparse incidence matrix\n",
    "        #edge_index, _ = to_edge_index(incidence_1)\n",
    "        y_hat = model(x_0, incidence_1)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            for x_0, incidence_1, y in zip(x_0_test, incidence_1_test, y_test):\n",
    "                x_0 = torch.tensor(x_0)\n",
    "                x_0, incidence_1, y = (\n",
    "                    x_0.float().to(device),\n",
    "                    incidence_1.float().to(device),\n",
    "                    torch.tensor(y, dtype=torch.float).to(device),\n",
    "                )\n",
    "                y_hat = model(x_0, incidence_1)\n",
    "                loss = loss_fn(y_hat, y)\n",
    "\n",
    "            print(f\"Test_loss: {loss:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
