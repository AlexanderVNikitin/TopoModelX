{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Complex Convolutional Network (SCCN)\n",
    "\n",
    "*TODO: more explanation, also in later cells. Refer to hsn example notebook*\n",
    "\n",
    "We train the model to perform binary node classification using the KarateClub benchmark dataset. \n",
    "\n",
    "The equations of one layer of this neural network are given by:\n",
    "\n",
    "*TODO*\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from toponetx import SimplicialComplex\n",
    "import toponetx.datasets.graph as graph\n",
    "\n",
    "from topomodelx.nn.simplicial.sccn_layer import SCCNLayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\", feat_dim=8)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 3  # There are features up to tetrahedron order in the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adjacency matrix H0 has shape: torch.Size([34, 34]).\n",
      "The adjacency matrix H1 has shape: torch.Size([78, 78]).\n",
      "The incidence matrix B1 has shape: torch.Size([34, 78]).\n",
      "The adjacency matrix H2 has shape: torch.Size([45, 45]).\n",
      "The incidence matrix B2 has shape: torch.Size([78, 45]).\n",
      "The adjacency matrix H3 has shape: torch.Size([11, 11]).\n",
      "The incidence matrix B3 has shape: torch.Size([45, 11]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavme15/miniconda3/envs/topoicml/lib/python3.10/site-packages/scipy/sparse/_index.py:143: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "def sparse_to_torch(X):\n",
    "    return torch.from_numpy(X.todense()).to_sparse()\n",
    "\n",
    "incidences = {\n",
    "    f'rank_{r}': sparse_to_torch(\n",
    "        dataset.incidence_matrix(rank=r)\n",
    "    )\n",
    "    for r in range(1, max_rank + 1)\n",
    "}\n",
    "\n",
    "adjacencies = {}\n",
    "adjacencies['rank_0'] = (sparse_to_torch(dataset.adjacency_matrix(rank=0)) \n",
    "                  + torch.eye(dataset.shape[0]).to_sparse())\n",
    "for r in range(1, max_rank):\n",
    "    adjacencies[f'rank_{r}'] = (\n",
    "        sparse_to_torch(\n",
    "            dataset.adjacency_matrix(rank=r)\n",
    "            + dataset.coadjacency_matrix(rank=r)\n",
    "        )\n",
    "        + 2 * torch.eye(dataset.shape[r]).to_sparse()\n",
    "    )\n",
    "adjacencies[f'rank_{max_rank}'] = (sparse_to_torch(dataset.coadjacency_matrix(rank=max_rank)) \n",
    "                         + torch.eye(dataset.shape[max_rank]).to_sparse())\n",
    "\n",
    "for r in range(max_rank + 1):\n",
    "    print(f\"The adjacency matrix H{r} has shape: {adjacencies[f'rank_{r}'].shape}.\")\n",
    "    if r > 0:\n",
    "        print(f\"The incidence matrix B{r} has shape: {incidences[f'rank_{r}'].shape}.\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "Since our task will be node classification, we must retrieve an input signal on the nodes. The signal will have shape $n_\\text{nodes} \\times$ in_channels, where in_channels is the dimension of each cell's feature. Here, we have in_channels = channels_nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "    x_1.append(v)\n",
    "x_1 = torch.tensor(np.stack(x_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78 edges with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for face features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "    x_2.append(v)\n",
    "x_2 = torch.tensor(np.stack(x_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45 faces with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_3 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"tetrahedron_feat\").items():\n",
    "    x_3.append(v)\n",
    "x_3 = torch.tensor(np.stack(x_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 tetrahedrons with features of dimension 8.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {x_3.shape[0]} tetrahedrons with features of dimension {x_3.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {'rank_0': x_0, 'rank_1': x_1, 'rank_2': x_2, 'rank_3': x_3}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We convert the binary labels into one-hot encoder form, and keep the last four nodes' true labels for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_test = y_true[-4:]\n",
    "y_train = y_true[:30]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the HSNLayer class, we create a neural network with stacked layers. A linear layer at the end produces an output with shape $n_\\text{nodes} \\times 2$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCCN(torch.nn.Module):\n",
    "    \"\"\"Simplicial Complex Convolutional Network Implementation for binary node classification.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    channels : int\n",
    "        Dimension of features\n",
    "    n_layers : int\n",
    "        Amount of message passing layers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, max_rank, n_layers=2, n_classes=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                SCCNLayer(\n",
    "                    channels=channels,\n",
    "                    max_rank=max_rank,\n",
    "                )\n",
    "            )\n",
    "        self.linear = torch.nn.Linear(channels, n_classes)\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, features, incidences, adjacencies):\n",
    "        \"\"\"Forward computation.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        TODO: same as in individual layers\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            shape = [n_nodes, 2]\n",
    "            One-hot labels assigned to nodes.\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            features = layer(features, incidences, adjacencies)\n",
    "        logits = self.linear(features['rank_0'])\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCCN(\n",
    "    channels=channels_nodes,\n",
    "    max_rank=max_rank,\n",
    "    n_layers=3,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 2 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 3 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 4 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 5 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 6 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 7 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 8 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 9 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 10 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 11 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 12 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 13 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 14 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 15 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 16 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 17 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 18 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 19 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 20 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 21 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 22 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 23 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 24 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 25 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 26 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 27 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 28 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 29 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 30 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 31 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 32 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 33 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 34 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 35 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 36 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 37 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 38 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 39 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 40 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 41 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 42 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 43 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 44 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 45 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 46 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 47 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 48 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 49 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 50 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 51 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 52 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 53 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 54 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 55 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 56 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 57 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 58 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 59 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 60 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 61 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 62 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 63 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 64 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 65 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 66 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 67 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 68 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 69 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 70 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 71 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 72 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 73 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 74 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 75 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 76 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 77 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 78 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 79 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 80 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 81 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 82 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 83 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 84 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 85 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 86 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 87 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 88 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 89 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 90 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 91 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 92 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 93 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 94 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 95 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 96 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 97 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 98 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 99 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 100 loss: 0.0000 Train_acc: 0.7000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 101 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 102 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 103 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 104 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 105 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 106 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 107 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 108 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 109 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 110 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 111 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 112 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 113 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 114 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 115 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 116 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 117 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 118 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 119 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 120 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 121 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 122 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 123 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 124 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 125 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 126 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 127 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 128 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 129 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 130 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 131 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 132 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 133 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 134 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 135 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 136 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 137 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 138 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 139 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 140 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 141 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 142 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 143 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 144 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 145 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 146 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 147 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 148 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 149 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 150 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 151 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 152 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 153 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 154 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 155 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 156 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 157 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 158 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 159 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 160 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 161 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 162 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 163 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 164 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 165 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 166 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 167 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 168 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 169 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 170 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 171 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 172 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 173 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 174 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 175 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 176 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 177 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 178 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 179 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 180 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 181 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 182 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 183 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 184 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 185 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 186 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 187 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 188 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 189 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 190 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 191 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 192 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 193 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 194 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 195 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 196 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 197 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 198 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 199 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 200 loss: 0.0000 Train_acc: 0.7000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 201 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 202 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 203 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 204 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 205 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 206 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 207 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 208 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 209 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 210 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 211 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 212 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 213 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 214 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 215 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 216 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 217 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 218 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 219 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 220 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 221 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 222 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 223 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 224 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 225 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 226 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 227 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 228 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 229 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 230 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 231 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 232 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 233 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 234 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 235 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 236 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 237 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 238 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 239 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 240 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 241 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 242 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 243 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 244 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 245 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 246 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 247 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 248 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 249 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 250 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 251 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 252 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 253 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 254 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 255 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 256 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 257 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 258 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 259 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 260 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 261 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 262 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 263 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 264 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 265 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 266 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 267 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 268 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 269 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 270 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 271 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 272 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 273 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 274 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 275 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 276 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 277 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 278 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 279 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 280 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 281 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 282 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 283 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 284 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 285 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 286 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 287 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 288 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 289 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 290 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 291 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 292 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 293 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 294 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 295 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 296 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 297 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 298 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 299 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 300 loss: 0.0000 Train_acc: 0.7000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 301 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 302 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 303 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 304 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 305 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 306 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 307 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 308 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 309 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 310 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 311 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 312 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 313 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 314 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 315 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 316 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 317 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 318 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 319 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 320 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 321 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 322 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 323 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 324 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 325 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 326 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 327 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 328 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 329 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 330 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 331 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 332 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 333 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 334 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 335 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 336 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 337 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 338 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 339 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 340 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 341 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 342 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 343 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 344 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 345 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 346 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 347 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 348 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 349 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 350 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 351 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 352 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 353 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 354 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 355 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 356 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 357 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 358 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 359 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 360 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 361 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 362 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 363 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 364 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 365 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 366 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 367 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 368 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 369 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 370 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 371 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 372 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 373 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 374 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 375 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 376 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 377 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 378 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 379 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 380 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 381 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 382 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 383 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 384 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 385 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 386 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 387 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 388 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 389 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 390 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 391 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 392 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 393 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 394 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 395 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 396 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 397 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 398 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 399 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 400 loss: 0.0000 Train_acc: 0.7000\n",
      "Test_acc: 0.7500\n",
      "Epoch: 401 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 402 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 403 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 404 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 405 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 406 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 407 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 408 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 409 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 410 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 411 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 412 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 413 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 414 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 415 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 416 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 417 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 418 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 419 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 420 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 421 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 422 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 423 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 424 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 425 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 426 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 427 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 428 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 429 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 430 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 431 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 432 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 433 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 434 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 435 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 436 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 437 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 438 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 439 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 440 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 441 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 442 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 443 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 444 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 445 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 446 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 447 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 448 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 449 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 450 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 451 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 452 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 453 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 454 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 455 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 456 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 457 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 458 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 459 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 460 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 461 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 462 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 463 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 464 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 465 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 466 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 467 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 468 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 469 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 470 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 471 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 472 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 473 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 474 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 475 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 476 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 477 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 478 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 479 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 480 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 481 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 482 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 483 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 484 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 485 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 486 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 487 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 488 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 489 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 490 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 491 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 492 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 493 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 494 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 495 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 496 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 497 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 498 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 499 loss: 0.0000 Train_acc: 0.7000\n",
      "Epoch: 500 loss: 0.0000 Train_acc: 0.7000\n",
      "Test_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "test_interval = 100\n",
    "num_epochs = 500\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(features, incidences, adjacencies)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[-len(y_train) :] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(features, incidences, adjacencies)\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[-len(y_test) :], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
