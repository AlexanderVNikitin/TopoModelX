{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Attention Network (SAN)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from toponetx import SimplicialComplex\n",
    "import toponetx.datasets.graph as graph\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "from topomodelx.nn.simplicial.san_layer import SANLayer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape [34, 78, 45, 11, 2] and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\")\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Now we retrieve the neighborhood structures (i.e. their representative matrices) that we will use to send messages on the domain. In this case, we need the down and upper laplacians of the nodes, $L_d^1=B_1^TB_1$ and $L_u^1=B_2B_2^T$ respectively, both with dimensions $n_\\text{edges} \\times n_\\text{edges}$. We also convert the neighborhood structures to torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ldown = torch.from_numpy(dataset.down_laplacian_matrix(rank=1).todense()).to_sparse()\n",
    "Lup = torch.from_numpy(dataset.up_laplacian_matrix(rank=1).todense()).to_sparse()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "The original task is node classification, but SAN work only with simplices of order higher or equal to 1 (edges, tetrahedrons, etc.). We will retrieve the input signal on both nodes and links, aggregate the information of nodes into link features and apply the SAN model on those resulting edge features. We will finally obtain the estimated node labels from the edge-level model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 2.\n",
      "There are 78 edges with features of dimension 2.\n",
      "There are 45 faces with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]\n",
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")\n",
    "\n",
    "x_1 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "    x_1.append(v)\n",
    "x_1 = torch.tensor(np.stack(x_1))\n",
    "print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")\n",
    "\n",
    "x_2 = []\n",
    "for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "    x_2.append(v)\n",
    "x_2 = np.stack(x_2)\n",
    "print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = x_1.shape[-1]\n",
    "hidden_channels = 16\n",
    "out_channels = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We convert the binary labels into one-hot encoder form, and keep the first four nodes' true labels for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_test = y_true[:4]\n",
    "y_train = y_true[-30:]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the HSNLayer class, we create a neural network with stacked layers. A linear layer at the end produces an output with shape $n_\\text{nodes} \\times 2$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAN(torch.nn.Module):\n",
    "    r\"\"\"Simplicial Attention Network (SAN) implementation for binary edge classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Dimension of input features.\n",
    "    hidden_channels : int\n",
    "        Dimension of hidden features.\n",
    "    out_channels : int\n",
    "        Dimension of output features.\n",
    "    num_filters_J : int, optional\n",
    "        Approximation order for simplicial filters. Defaults to 2.\n",
    "    J_har : int, optional\n",
    "        Approximation order for harmonic convolution. Defaults to 5.\n",
    "    epsilon_har : float, optional\n",
    "        Epsilon value for harmonic convolution. Defaults to 1e-1.\n",
    "    n_layers : int, optional\n",
    "        Number of message passing layers. Defaults to 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_filters_J=2,\n",
    "        J_har=5,\n",
    "        epsilon_har=1e-1,\n",
    "        n_layers=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_filters_J = num_filters_J\n",
    "        self.J_har = J_har\n",
    "        self.epsilon_har = epsilon_har\n",
    "        if n_layers == 1:\n",
    "            self.layers = [\n",
    "                SANLayer(\n",
    "                    in_channels=self.in_channels,\n",
    "                    out_channels=self.out_channels,\n",
    "                    num_filters_J=self.num_filters_J,\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            self.layers = [\n",
    "                SANLayer(\n",
    "                    in_channels=self.in_channels,\n",
    "                    out_channels=self.hidden_channels,\n",
    "                    num_filters_J=self.num_filters_J,\n",
    "                )\n",
    "            ]\n",
    "            for _ in range(n_layers - 2):\n",
    "                self.layers.append(\n",
    "                    SANLayer(\n",
    "                        in_channels=self.hidden_channels,\n",
    "                        out_channels=self.hidden_channels,\n",
    "                        num_filters_J=self.num_filters_J,\n",
    "                    )\n",
    "                )\n",
    "            self.layers.append(\n",
    "                SANLayer(\n",
    "                    in_channels=self.hidden_channels,\n",
    "                    out_channels=self.out_channels,\n",
    "                    num_filters_J=self.num_filters_J,\n",
    "                )\n",
    "            )\n",
    "        self.linear = torch.nn.Linear(out_channels, 2)\n",
    "\n",
    "    def compute_projection_matrix(self, L):\n",
    "        r\"\"\"Computation of the projection matrix which is then used\n",
    "        to calculate the harmonic component in SAN layers.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "\n",
    "        L : tensor\n",
    "            shape = [n_edges, n_edges]\n",
    "            Hodge laplacian of rank 1.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            shape = [n_edges, n_edges]\n",
    "            Projection matrix.\n",
    "\n",
    "        \"\"\"\n",
    "        P = torch.eye(L.shape[0]) - self.epsilon_har * L\n",
    "        P = torch.linalg.matrix_power(P, self.J_har)\n",
    "        return P\n",
    "\n",
    "    def forward(self, x, Lup, Ldown):\n",
    "        r\"\"\"Forward computation.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        x : tensor\n",
    "            shape = [n_nodes, channels_in]\n",
    "            Node features.\n",
    "\n",
    "        Lup : tensor\n",
    "            shape = [n_edges, n_edges]\n",
    "            Upper laplacian of rank 1.\n",
    "\n",
    "        Ld : tensor\n",
    "            shape = [n_edges, n_edges]\n",
    "            Down laplacian of rank 1.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            shape = [n_nodes, 2]\n",
    "            One-hot labels assigned to edges.\n",
    "\n",
    "        \"\"\"\n",
    "        # Compute the projection matrix for the harmonic component\n",
    "        L = Lup + Ldown\n",
    "        P = self.compute_projection_matrix(L)\n",
    "\n",
    "        # Forward computation\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, Lup, Ldown, P)\n",
    "        return torch.sigmoid(self.linear(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SANConv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m SAN(\n\u001b[1;32m      2\u001b[0m     in_channels\u001b[39m=\u001b[39;49min_channels,\n\u001b[1;32m      3\u001b[0m     hidden_channels\u001b[39m=\u001b[39;49mhidden_channels,\n\u001b[1;32m      4\u001b[0m     out_channels\u001b[39m=\u001b[39;49mout_channels,\n\u001b[1;32m      5\u001b[0m     n_layers\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m)\n\u001b[1;32m      9\u001b[0m test_interval \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n",
      "Cell \u001b[0;32mIn[21], line 41\u001b[0m, in \u001b[0;36mSAN.__init__\u001b[0;34m(self, in_channels, hidden_channels, out_channels, num_filters_J, J_har, epsilon_har, n_layers)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon_har \u001b[39m=\u001b[39m epsilon_har\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m n_layers \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m [\n\u001b[0;32m---> 41\u001b[0m         SANLayer(\n\u001b[1;32m     42\u001b[0m             in_channels\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_channels, \n\u001b[1;32m     43\u001b[0m             out_channels\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_channels,\n\u001b[1;32m     44\u001b[0m             num_filters_J\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_filters_J\n\u001b[1;32m     45\u001b[0m         )\n\u001b[1;32m     46\u001b[0m     ]\n\u001b[1;32m     47\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m         SANLayer(\n\u001b[1;32m     50\u001b[0m             in_channels\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_channels, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/Cambridge/TopoModelX/topomodelx/nn/simplicial/san_layer.py:47\u001b[0m, in \u001b[0;36mSANLayer.__init__\u001b[0;34m(self, in_channels, out_channels, num_filters_J)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_filters_J \u001b[39m=\u001b[39m num_filters_J\n\u001b[1;32m     45\u001b[0m \u001b[39m#  Convolutions\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# Down convolutions, one for each filter order p\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_down \u001b[39m=\u001b[39m SANConv(in_channels, out_channels, num_filters_J)\n\u001b[1;32m     49\u001b[0m \u001b[39m# Up convolutions, one for each filter order p\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_up \u001b[39m=\u001b[39m SANConv(in_channels, out_channels, num_filters_J)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SANConv' is not defined"
     ]
    }
   ],
   "source": [
    "model = SAN(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    n_layers=1,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.4)\n",
    "test_interval = 2\n",
    "num_epochs = 5\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(x_1, Lup=Lup, Ldown=Ldown)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[-len(y_train) :].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred == y_hat).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(x_1, Lup=Lup, Ldown=Ldown)\n",
    "            y_pred_test = torch.sigmoid(y_hat_test).ge(0.5).float()\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[: len(y_test)], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(x_1, Lup=Lup, Ldown=Ldown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_irr = Parameter(torch.Tensor(J, channels_nodes, output_dim))\n",
    "att_irr = Parameter(torch.Tensor(2 * att_slice, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_irr = torch.matmul(x_1, weight_irr).reshape(-1, J * output_dim)\n",
    "x_irr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_irr @ att_irr[:att_slice, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_irr @ att_irr[:att_slice, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 78])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_irr @ att_irr[att_slice:, :]).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_irr = (x_irr @ att_irr[:att_slice, :]) + (x_irr @ att_irr[att_slice:, :]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_irr @ att_irr[:att_slice, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 78])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_irr @ att_irr[att_slice:, :]).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x_irr, att_irr[:att_slice, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 78])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x_irr, att_irr[att_slice:, :]).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_irr = torch.sparse.softmax(e_irr.sparse_mask(Ldown), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  0,  0,  ..., 77, 77, 77],\n",
       "                       [ 0,  1,  2,  ..., 75, 76, 77]]),\n",
       "       values=tensor([0.0417, 0.0417, 0.0417,  ..., 0.0455, 0.0455, 0.0455]),\n",
       "       size=(78, 78), nnz=1134, layout=torch.sparse_coo,\n",
       "       grad_fn=<SparseSoftmaxBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_irr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_exp_irr = alpha_irr.unsqueeze(0)\n",
    "for p in range(J - 1):\n",
    "    alpha_exp_irr = torch.cat(\n",
    "        [alpha_exp_irr, torch.mm(alpha_exp_irr[p], alpha_irr).unsqueeze(0)], dim=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  0,  0,  ...,  2,  2,  2],\n",
       "                       [ 0,  0,  0,  ..., 77, 77, 77],\n",
       "                       [ 0,  1,  2,  ..., 75, 76, 77]]),\n",
       "       values=tensor([0.0417, 0.0417, 0.0417,  ..., 0.0376, 0.0368, 0.0476]),\n",
       "       size=(3, 78, 78), nnz=10742, layout=torch.sparse_coo,\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_exp_irr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 78, 2])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_irr = torch.matmul(x_1, weight_irr)\n",
    "x_irr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 2])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.matmul(alpha_exp_irr.to_dense(), x_irr), dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  0,  0,  ..., 77, 77, 77],\n",
       "                       [ 0,  1,  2,  ..., 75, 76, 77]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(78, 78), nnz=1134, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sparse_mask(Ldown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = SANLayer(channels_in=channels_nodes, channels_out=output_dim, J=J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 2])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x_1, Lup, Ldown, Lup).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = channels_nodes\n",
    "out_channels = output_dim\n",
    "num_filters_J = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topomodelx.base.aggregation import Aggregation\n",
    "from topomodelx.base.conv import Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SANConv(Conv):\n",
    "    r\"\"\"Class for the SAN Convolution\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        p_filters,\n",
    "        initialization=\"xavier_uniform\",\n",
    "    ):\n",
    "        super(Conv, self).__init__(\n",
    "            att=True,\n",
    "            initialization=initialization,\n",
    "        )\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.p_filters = p_filters\n",
    "        self.initialization = initialization\n",
    "\n",
    "        self.weight = Parameter(\n",
    "            torch.Tensor(self.p_filters, self.in_channels, self.out_channels)\n",
    "        )\n",
    "\n",
    "        self.att_weight = Parameter(\n",
    "            torch.Tensor(\n",
    "                2 * self.out_channels * self.p_filters,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x_source, neighborhood):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        This implements message passing:\n",
    "        - from source cells with input features `x_source`,\n",
    "        - via `neighborhood` defining where messages can pass,\n",
    "        - to target cells, which are the same source cells.\n",
    "\n",
    "        In practice, this will update the features on the target cells.\n",
    "\n",
    "        If not provided, x_target is assumed to be x_source,\n",
    "        i.e. source cells send messages to themselves.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_source : Tensor, shape=[..., n_source_cells, in_channels]\n",
    "            Input features on source cells.\n",
    "            Assumes that all source cells have the same rank r.\n",
    "        neighborhood : torch.sparse, shape=[n_target_cells, n_source_cells]\n",
    "            Neighborhood matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _ : Tensor, shape=[..., n_target_cells, out_channels]\n",
    "            Output features on target cells.\n",
    "            Assumes that all target cells have the same rank s.\n",
    "        \"\"\"\n",
    "        x_message = torch.matmul(x_source, self.weight)\n",
    "        # Reshape required to re-use the attention function of parent Conv class\n",
    "        # -> [num_nodes, out_channels * p_filters]\n",
    "        x_message_reshaped = x_message.permute(1, 0, 2).reshape(\n",
    "            -1, self.out_channels * self.p_filters\n",
    "        )\n",
    "\n",
    "        # SAN always requires attention\n",
    "        # In SAN, neighborhood is defined by lower/upper laplacians; we only use them as masks\n",
    "        # to keep only the relevant attention coeffs\n",
    "        neighborhood = neighborhood.coalesce()\n",
    "        self.target_index_i, self.source_index_j = neighborhood.indices()\n",
    "        attention_values = self.attention(x_message_reshaped)\n",
    "        att_laplacian = torch.sparse_coo_tensor(\n",
    "            indices=neighborhood.indices(),\n",
    "            values=attention_values,\n",
    "            size=neighborhood.shape,\n",
    "        )\n",
    "\n",
    "        # Attention coeffs are normalized using softmax\n",
    "        att_laplacian = torch.sparse.softmax(att_laplacian, dim=1)\n",
    "        # We need to compute the power of the attention laplacian according to the filter order p\n",
    "        att_laplacian_power = torch.stack(\n",
    "            [\n",
    "                torch.linalg.matrix_power(att_laplacian, p + 1)\n",
    "                if p > 1\n",
    "                else att_laplacian\n",
    "                for p in range(1, self.p_filters + 1)\n",
    "            ]\n",
    "        ).to_dense()\n",
    "\n",
    "        # When computing the final message on targets, we need to compute the power of the attention laplacian\n",
    "        # according to the filter order p\n",
    "        x_message_on_target = torch.matmul(att_laplacian_power, x_message).sum(dim=0)\n",
    "\n",
    "        return x_message_on_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanconv = SANConv(\n",
    "    in_channels=in_channels, out_channels=out_channels, p_filters=num_filters_J\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_lap = sanconv(x_1, Lup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_lap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs_irr = [SANConv(in_channels, out_channels, p) for p in range(num_filters_J)]\n",
    "convs_sol = [SANConv(in_channels, out_channels, p) for p in range(num_filters_J)]\n",
    "conv_har = Conv(in_channels, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'P' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m z_irr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([conv(x_1, Ldown) \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m convs_irr])\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m z_sol \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([conv(x_1, Lup) \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m convs_sol])\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m z_har \u001b[39m=\u001b[39m conv_har(x_1, P)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'P' is not defined"
     ]
    }
   ],
   "source": [
    "z_irr = torch.stack([conv(x_1, Ldown) for conv in convs_irr]).sum(dim=0)\n",
    "z_sol = torch.stack([conv(x_1, Lup) for conv in convs_sol]).sum(dim=0)\n",
    "z_har = conv_har(x_1, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0004,  0.0030],\n",
       "        [ 0.0038, -0.0032],\n",
       "        [-0.0014,  0.0211],\n",
       "        [ 0.0010,  0.0031],\n",
       "        [ 0.0013,  0.0038],\n",
       "        [ 0.0002,  0.0057],\n",
       "        [ 0.0195, -0.0203],\n",
       "        [ 0.0034,  0.0041],\n",
       "        [ 0.0016,  0.0020],\n",
       "        [ 0.0013,  0.0027],\n",
       "        [-0.0266,  0.0401],\n",
       "        [ 0.0019,  0.0081],\n",
       "        [ 0.0018,  0.0024],\n",
       "        [ 0.0158, -0.0140],\n",
       "        [ 0.0018,  0.0024],\n",
       "        [-0.0006,  0.0011],\n",
       "        [ 0.0018, -0.0033],\n",
       "        [-0.0070,  0.0321],\n",
       "        [ 0.0261, -0.0297],\n",
       "        [-0.0020,  0.0137],\n",
       "        [-0.0033,  0.0058],\n",
       "        [ 0.0206, -0.0203],\n",
       "        [-0.0033,  0.0058],\n",
       "        [-0.0002,  0.0114],\n",
       "        [-0.0040,  0.0236],\n",
       "        [ 0.0268, -0.0354],\n",
       "        [ 0.0043,  0.0022],\n",
       "        [ 0.0066, -0.0129],\n",
       "        [-0.0003, -0.0007],\n",
       "        [-0.0097,  0.0384],\n",
       "        [-0.0042,  0.0695],\n",
       "        [-0.0027,  0.0474],\n",
       "        [-0.0024,  0.0114],\n",
       "        [-0.0029,  0.0138],\n",
       "        [ 0.0015,  0.0017],\n",
       "        [-0.0074,  0.0315],\n",
       "        [ 0.0039,  0.0094],\n",
       "        [-0.0074,  0.0558],\n",
       "        [ 0.0048, -0.0040],\n",
       "        [-0.0081,  0.0618],\n",
       "        [ 0.0016,  0.0009],\n",
       "        [ 0.0007,  0.0048],\n",
       "        [-0.0042,  0.0163],\n",
       "        [-0.0060,  0.0206],\n",
       "        [ 0.0066, -0.0414],\n",
       "        [ 0.0041, -0.0248],\n",
       "        [ 0.0174, -0.0342],\n",
       "        [ 0.0144, -0.0400],\n",
       "        [ 0.0051, -0.0231],\n",
       "        [ 0.0216, -0.0189],\n",
       "        [ 0.0216, -0.0367],\n",
       "        [ 0.0121, -0.0138],\n",
       "        [ 0.0148, -0.0299],\n",
       "        [ 0.0042, -0.0005],\n",
       "        [ 0.0053,  0.0091],\n",
       "        [ 0.0057,  0.0085],\n",
       "        [ 0.0067,  0.0074],\n",
       "        [-0.0097,  0.0267],\n",
       "        [-0.0148,  0.0341],\n",
       "        [ 0.0086, -0.0072],\n",
       "        [ 0.0046,  0.0016],\n",
       "        [-0.0036,  0.0200],\n",
       "        [ 0.0086, -0.0047],\n",
       "        [-0.0044,  0.0425],\n",
       "        [-0.0056,  0.0560],\n",
       "        [-0.0082,  0.0625],\n",
       "        [-0.0076,  0.0618],\n",
       "        [-0.0082,  0.0624],\n",
       "        [-0.0041,  0.0519],\n",
       "        [-0.0075,  0.0635],\n",
       "        [-0.0161,  0.0741],\n",
       "        [-0.0030,  0.0423],\n",
       "        [-0.0089,  0.0611],\n",
       "        [-0.0041,  0.0482],\n",
       "        [-0.0059,  0.0559],\n",
       "        [-0.0011,  0.0454],\n",
       "        [-0.0072,  0.0592],\n",
       "        [-0.0031,  0.0427]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
