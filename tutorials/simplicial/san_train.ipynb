{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Attention Network (SAN)\n",
    "\n",
    "In this notebook, we will create and train a High Skip Network in the simplicial complex domain, as proposed in the paper by [Hajij et. al : High Skip Networks: A Higher Order Generalization of Skip Connections (2022)](https://openreview.net/pdf?id=Sc8glB-k6e9). \n",
    "\n",
    "We train the model to perform binary node classification using the KarateClub benchmark dataset. \n",
    "\n",
    "The equations of one layer of this neural network are given by:\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow z}}^{(0 \\rightarrow 0)} = \\sigma ((A_{\\uparrow,0})_{xy} \\cdot h^{t,(0)}_y \\cdot \\Theta^{t,(0)1})$    (level 1)\n",
    "\n",
    "游린 $\\quad m_{z \\rightarrow x}^{(0 \\rightarrow 0)}  = (A_{\\uparrow,0})_{xy} \\cdot m_{y \\rightarrow z}^{(0 \\rightarrow 0)} \\cdot \\Theta^{t,(0)2}$    (level 2)\n",
    "\n",
    "游린 $\\quad m_{{y \\rightarrow z}}^{(0 \\rightarrow 1)}  = \\sigma((B_1^T)_{zy} \\cdot h_y^{t,(0)} \\cdot \\Theta^{t,(0 \\rightarrow 1)})$    (level 1)\n",
    "\n",
    "游린 $\\quad m_{z \\rightarrow x)}^{(1 \\rightarrow 0)}  = (B_1)_{xz} \\cdot m_{z \\rightarrow x}^{(0 \\rightarrow 1)} \\cdot \\Theta^{t, (1 \\rightarrow 0)}$    (level 2)\n",
    "\n",
    "游릲 $\\quad m_{x}^{(0 \\rightarrow 0)}  = \\sum_{z \\in \\mathcal{L}_\\uparrow(x)} m_{z \\rightarrow x}^{(0 \\rightarrow 0)}$\n",
    "\n",
    "游릲 $\\quad m_{x}^{(1 \\rightarrow 0)}  = \\sum_{z \\in \\mathcal{C}(x)} m_{z \\rightarrow x}^{(1 \\rightarrow 0)}$\n",
    "\n",
    "游릴 $\\quad m_x^{(0)}  = m_x^{(0 \\rightarrow 0)} + m_x^{(1 \\rightarrow 0)}$\n",
    "\n",
    "游릱 $\\quad h_x^{t+1,(0)}  = I(m_x^{(0)})$\n",
    "\n",
    "Where the notations are defined in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_geometric/typing.py:33: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: dlopen(/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_scatter/_version_cpu.so, 0x0006): tried: '/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_scatter/_version_cpu.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e'))\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_geometric/typing.py:64: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_sparse/_version_cpu.so, 0x0006): tried: '/Users/leone/anaconda3/envs/topo/lib/python3.11/site-packages/torch_sparse/_version_cpu.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e'))\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from toponetx import SimplicialComplex\n",
    "import toponetx.datasets.graph as graph\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "from topomodelx.nn.simplicial.san_layer import SANLayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape [34, 78, 45, 11, 2] and dimension 4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\")\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures. ##\n",
    "\n",
    "Now we retrieve the neighborhood structures (i.e. their representative matrices) that we will use to send messges on the domain. In this case, we need the boundary matrix (or incidence matrix) $B_1$ and the adjacency matrix $A_{\\uparrow,0}$ on the nodes. For a santiy check, we show that the shape of the $B_1 = n_\\text{nodes} \\times n_\\text{edges}$ and $A_{\\uparrow,0} = n_\\text{nodes} \\times n_\\text{nodes}$. We also convert the neighborhood structures to torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The incidence matrix B2 has shape: torch.Size([78, 45]).\n",
      "The incidence matrix B1 has shape: torch.Size([34, 78]).\n",
      "The adjacency matrix A0 has shape: torch.Size([34, 34]).\n"
     ]
    }
   ],
   "source": [
    "# incidence_2 = dataset.incidence_matrix(rank=2)\n",
    "# incidence_1 = dataset.incidence_matrix(rank=1)\n",
    "# adjacency_0 = dataset.adjacency_matrix(rank=0)\n",
    "\n",
    "# incidence_2 = torch.from_numpy(incidence_2.todense())#.to_sparse()\n",
    "# incidence_1 = torch.from_numpy(incidence_1.todense())#.to_sparse()\n",
    "# adjacency_0 = torch.from_numpy(adjacency_0.todense())#.to_sparse()\n",
    "\n",
    "# print(f\"The incidence matrix B2 has shape: {incidence_2.shape}.\")\n",
    "# print(f\"The incidence matrix B1 has shape: {incidence_1.shape}.\")\n",
    "# print(f\"The adjacency matrix A0 has shape: {adjacency_0.shape}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "Since our task will be node classification, we must retrieve an input signal on the nodes. The signal will have shape $n_\\text{nodes} \\times$ in_channels, where in_channels is the dimension of each cell's feature. Here, we have in_channels = channels_nodes $ = 34$. This is because the Karate dataset encodes the identity of each of the 34 nodes as a one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 nodes with features of dimension 2.\n"
     ]
    }
   ],
   "source": [
    "x_0 = []\n",
    "for _, v in dataset.get_simplex_attributes(\"node_feat\").items():\n",
    "    x_0.append(v)\n",
    "x_0 = torch.tensor(np.stack(x_0))\n",
    "channels_nodes = x_0.shape[-1]\n",
    "print(f\"There are {x_0.shape[0]} nodes with features of dimension {x_0.shape[1]}.\")\n",
    "\n",
    "# x_1 = []\n",
    "# for k, v in dataset.get_simplex_attributes(\"edge_feat\").items():\n",
    "#     x_1.append(v)\n",
    "# x_1 = np.stack(x_1)\n",
    "# print(f\"There are {x_1.shape[0]} edges with features of dimension {x_1.shape[1]}.\")\n",
    "\n",
    "# x_2 = []\n",
    "# for k, v in dataset.get_simplex_attributes(\"face_feat\").items():\n",
    "#     x_2.append(v)\n",
    "# x_2 = np.stack(x_2)\n",
    "# print(f\"There are {x_2.shape[0]} faces with features of dimension {x_2.shape[1]}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We convert the binary labels into one-hot encoder form, and keep the first four nodes' true labels for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_test = y_true[:4]\n",
    "y_train = y_true[-30:]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the HSNLayer class, we create a neural network with stacked layers. A linear layer at the end produces an output with shape $n_\\text{nodes} \\times 2$, so we can compare with our binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SAN(torch.nn.Module):\n",
    "    \"\"\"Simplicial Attention Network Implementation for binary node classification.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    channels : int\n",
    "        Dimension of features\n",
    "    n_layers : int\n",
    "        Amount of message passing layers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels_in, channels_out, J=2, n_layers=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                SANLayer(\n",
    "                    channels_in=channels_in,\n",
    "                    channels_out=channels_out,\n",
    "                    J=J,\n",
    "                )\n",
    "            )\n",
    "        self.linear = torch.nn.Linear(channels_out, 2)\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x, Lup, Ld, P):\n",
    "        \n",
    "        \"\"\"Forward computation.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        x : tensor\n",
    "            shape = [n_nodes, channels_in]\n",
    "            Node features.\n",
    "\n",
    "        Lup : tensor\n",
    "            shape = [n_nodes, n_edges]\n",
    "            Boundary matrix of rank 1.\n",
    "\n",
    "        Ld : tensor\n",
    "            shape = [n_nodes, n_nodes]\n",
    "            Adjacency matrix (up) of rank 0.\n",
    "        \n",
    "        P : tensor\n",
    "            shape = [n_nodes, n_nodes]\n",
    "            \n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        _ : tensor\n",
    "            shape = [n_nodes, 2]\n",
    "            One-hot labels assigned to nodes.\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, Lup, Ld, P)\n",
    "        return torch.sigmoid(self.linear(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAN(\n",
    "    channels_in=channels_nodes,\n",
    "    channels_out=channels_nodes,\n",
    "    n_layers=3,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_projection_matrix(L, eps, order):\n",
    "    P = (torch.eye(L.shape[0]) - eps*L)\n",
    "    for _ in range(order):\n",
    "        P = P @ P  # approximate the limit\n",
    "    return P\n",
    "\n",
    "Ld = dataset.down_laplacian_matrix(rank=1)\n",
    "Lup = dataset.up_laplacian_matrix(rank=1)\n",
    "\n",
    "Ld = torch.from_numpy(Ld.todense())#.to_sparse()\n",
    "Lup = torch.from_numpy(Lup.todense())#.to_sparse()\n",
    "\n",
    "# eps take into account the normalization \n",
    "L = Ld + Lup\n",
    "P = compute_projection_matrix(L, eps=0.1, order=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs the training, looping over the network for a low number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SANLayer' object has no attribute 'leakyrelu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m y_hat \u001b[39m=\u001b[39m model(x_0, Lup\u001b[39m=\u001b[39;49mLup, Ld\u001b[39m=\u001b[39;49mLd, P\u001b[39m=\u001b[39;49mP)\n\u001b[1;32m      9\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m     10\u001b[0m     y_hat[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(y_train) :]\u001b[39m.\u001b[39mfloat(), y_train\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m epoch_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/topo/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 57\u001b[0m, in \u001b[0;36mSAN.forward\u001b[0;34m(self, x, Lup, Ld, P)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Forward computation.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 57\u001b[0m     x \u001b[39m=\u001b[39m layer(x, Lup, Ld, P)\n\u001b[1;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/topo/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/PhD-S/projects/TopoModelX/topomodelx/nn/simplicial/san_layer.py:94\u001b[0m, in \u001b[0;36mSANLayer.forward\u001b[0;34m(self, x, Lup, Ld, P)\u001b[0m\n\u001b[1;32m     87\u001b[0m x_sol \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_sol[i](x) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mJ)], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[39m# Broadcast add\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m# Attention map\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[39m# (Ex1) + (1xE) -> (ExE)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m E_irr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mleakyrelu((x_irr \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_irr[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_slice, :]) \u001b[39m+\u001b[39m \n\u001b[1;32m     95\u001b[0m                         (x_irr \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_irr[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_slice:, :])\u001b[39m.\u001b[39mT) \n\u001b[1;32m     97\u001b[0m \u001b[39m# (Ex1) + (1xE) -> (ExE) \u001b[39;00m\n\u001b[1;32m     98\u001b[0m E_sol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleakyrelu((x_sol \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_sol[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_slice, :]) \u001b[39m+\u001b[39m \n\u001b[1;32m     99\u001b[0m                         (x_sol \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_sol[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_slice:, :])\u001b[39m.\u001b[39mT)  \n",
      "File \u001b[0;32m~/anaconda3/envs/topo/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SANLayer' object has no attribute 'leakyrelu'"
     ]
    }
   ],
   "source": [
    "test_interval = 2\n",
    "num_epochs = 5\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_hat = model(x_0, Lup=Lup, Ld=Ld, P=P)\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        y_hat[-len(y_train) :].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred == y_hat).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(x_0, incidence_1, adjacency_0)\n",
    "            y_pred_test = torch.sigmoid(y_hat_test).ge(0.5).float()\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[: len(y_test)], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
